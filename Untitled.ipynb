{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24a98b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70684dd",
   "metadata": {},
   "source": [
    "# Renommer les sous dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae92156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dossier n02085620-Chihuahua a été renommé en Chihuahua.\n",
      "Le dossier n02085782-Japanese_spaniel a été renommé en Japanese_spaniel.\n",
      "Le dossier n02085936-Maltese_dog a été renommé en Maltese_dog.\n",
      "Le dossier n02086079-Pekinese a été renommé en Pekinese.\n",
      "Le dossier n02086240-Shih-Tzu a été renommé en Shih-Tzu.\n",
      "Le dossier n02086646-Blenheim_spaniel a été renommé en Blenheim_spaniel.\n",
      "Le dossier n02086910-papillon a été renommé en papillon.\n",
      "Le dossier n02087046-toy_terrier a été renommé en toy_terrier.\n",
      "Le dossier n02087394-Rhodesian_ridgeback a été renommé en Rhodesian_ridgeback.\n",
      "Le dossier n02088094-Afghan_hound a été renommé en Afghan_hound.\n",
      "Le dossier n02088238-basset a été renommé en basset.\n",
      "Le dossier n02088364-beagle a été renommé en beagle.\n",
      "Le dossier n02088466-bloodhound a été renommé en bloodhound.\n",
      "Le dossier n02088632-bluetick a été renommé en bluetick.\n",
      "Le dossier n02089078-black-and-tan_coonhound a été renommé en black-and-tan_coonhound.\n",
      "Le dossier n02089867-Walker_hound a été renommé en Walker_hound.\n",
      "Le dossier n02089973-English_foxhound a été renommé en English_foxhound.\n",
      "Le dossier n02090379-redbone a été renommé en redbone.\n",
      "Le dossier n02090622-borzoi a été renommé en borzoi.\n",
      "Le dossier n02090721-Irish_wolfhound a été renommé en Irish_wolfhound.\n",
      "Le dossier n02091032-Italian_greyhound a été renommé en Italian_greyhound.\n",
      "Le dossier n02091134-whippet a été renommé en whippet.\n",
      "Le dossier n02091244-Ibizan_hound a été renommé en Ibizan_hound.\n",
      "Le dossier n02091467-Norwegian_elkhound a été renommé en Norwegian_elkhound.\n",
      "Le dossier n02091635-otterhound a été renommé en otterhound.\n",
      "Le dossier n02091831-Saluki a été renommé en Saluki.\n",
      "Le dossier n02092002-Scottish_deerhound a été renommé en Scottish_deerhound.\n",
      "Le dossier n02092339-Weimaraner a été renommé en Weimaraner.\n",
      "Le dossier n02093256-Staffordshire_bullterrier a été renommé en Staffordshire_bullterrier.\n",
      "Le dossier n02093428-American_Staffordshire_terrier a été renommé en American_Staffordshire_terrier.\n",
      "Le dossier n02093647-Bedlington_terrier a été renommé en Bedlington_terrier.\n",
      "Le dossier n02093754-Border_terrier a été renommé en Border_terrier.\n",
      "Le dossier n02093859-Kerry_blue_terrier a été renommé en Kerry_blue_terrier.\n",
      "Le dossier n02093991-Irish_terrier a été renommé en Irish_terrier.\n",
      "Le dossier n02094114-Norfolk_terrier a été renommé en Norfolk_terrier.\n",
      "Le dossier n02094258-Norwich_terrier a été renommé en Norwich_terrier.\n",
      "Le dossier n02094433-Yorkshire_terrier a été renommé en Yorkshire_terrier.\n",
      "Le dossier n02095314-wire-haired_fox_terrier a été renommé en wire-haired_fox_terrier.\n",
      "Le dossier n02095570-Lakeland_terrier a été renommé en Lakeland_terrier.\n",
      "Le dossier n02095889-Sealyham_terrier a été renommé en Sealyham_terrier.\n",
      "Le dossier n02096051-Airedale a été renommé en Airedale.\n",
      "Le dossier n02096177-cairn a été renommé en cairn.\n",
      "Le dossier n02096294-Australian_terrier a été renommé en Australian_terrier.\n",
      "Le dossier n02096437-Dandie_Dinmont a été renommé en Dandie_Dinmont.\n",
      "Le dossier n02096585-Boston_bull a été renommé en Boston_bull.\n",
      "Le dossier n02097047-miniature_schnauzer a été renommé en miniature_schnauzer.\n",
      "Le dossier n02097130-giant_schnauzer a été renommé en giant_schnauzer.\n",
      "Le dossier n02097209-standard_schnauzer a été renommé en standard_schnauzer.\n",
      "Le dossier n02097298-Scotch_terrier a été renommé en Scotch_terrier.\n",
      "Le dossier n02097474-Tibetan_terrier a été renommé en Tibetan_terrier.\n",
      "Le dossier n02097658-silky_terrier a été renommé en silky_terrier.\n",
      "Le dossier n02098105-soft-coated_wheaten_terrier a été renommé en soft-coated_wheaten_terrier.\n",
      "Le dossier n02098286-West_Highland_white_terrier a été renommé en West_Highland_white_terrier.\n",
      "Le dossier n02098413-Lhasa a été renommé en Lhasa.\n",
      "Le dossier n02099267-flat-coated_retriever a été renommé en flat-coated_retriever.\n",
      "Le dossier n02099429-curly-coated_retriever a été renommé en curly-coated_retriever.\n",
      "Le dossier n02099601-golden_retriever a été renommé en golden_retriever.\n",
      "Le dossier n02099712-Labrador_retriever a été renommé en Labrador_retriever.\n",
      "Le dossier n02099849-Chesapeake_Bay_retriever a été renommé en Chesapeake_Bay_retriever.\n",
      "Le dossier n02100236-German_short-haired_pointer a été renommé en German_short-haired_pointer.\n",
      "Le dossier n02100583-vizsla a été renommé en vizsla.\n",
      "Le dossier n02100735-English_setter a été renommé en English_setter.\n",
      "Le dossier n02100877-Irish_setter a été renommé en Irish_setter.\n",
      "Le dossier n02101006-Gordon_setter a été renommé en Gordon_setter.\n",
      "Le dossier n02101388-Brittany_spaniel a été renommé en Brittany_spaniel.\n",
      "Le dossier n02101556-clumber a été renommé en clumber.\n",
      "Le dossier n02102040-English_springer a été renommé en English_springer.\n",
      "Le dossier n02102177-Welsh_springer_spaniel a été renommé en Welsh_springer_spaniel.\n",
      "Le dossier n02102318-cocker_spaniel a été renommé en cocker_spaniel.\n",
      "Le dossier n02102480-Sussex_spaniel a été renommé en Sussex_spaniel.\n",
      "Le dossier n02102973-Irish_water_spaniel a été renommé en Irish_water_spaniel.\n",
      "Le dossier n02104029-kuvasz a été renommé en kuvasz.\n",
      "Le dossier n02104365-schipperke a été renommé en schipperke.\n",
      "Le dossier n02105056-groenendael a été renommé en groenendael.\n",
      "Le dossier n02105162-malinois a été renommé en malinois.\n",
      "Le dossier n02105251-briard a été renommé en briard.\n",
      "Le dossier n02105412-kelpie a été renommé en kelpie.\n",
      "Le dossier n02105505-komondor a été renommé en komondor.\n",
      "Le dossier n02105641-Old_English_sheepdog a été renommé en Old_English_sheepdog.\n",
      "Le dossier n02105855-Shetland_sheepdog a été renommé en Shetland_sheepdog.\n",
      "Le dossier n02106030-collie a été renommé en collie.\n",
      "Le dossier n02106166-Border_collie a été renommé en Border_collie.\n",
      "Le dossier n02106382-Bouvier_des_Flandres a été renommé en Bouvier_des_Flandres.\n",
      "Le dossier n02106550-Rottweiler a été renommé en Rottweiler.\n",
      "Le dossier n02106662-German_shepherd a été renommé en German_shepherd.\n",
      "Le dossier n02107142-Doberman a été renommé en Doberman.\n",
      "Le dossier n02107312-miniature_pinscher a été renommé en miniature_pinscher.\n",
      "Le dossier n02107574-Greater_Swiss_Mountain_dog a été renommé en Greater_Swiss_Mountain_dog.\n",
      "Le dossier n02107683-Bernese_mountain_dog a été renommé en Bernese_mountain_dog.\n",
      "Le dossier n02107908-Appenzeller a été renommé en Appenzeller.\n",
      "Le dossier n02108000-EntleBucher a été renommé en EntleBucher.\n",
      "Le dossier n02108089-boxer a été renommé en boxer.\n",
      "Le dossier n02108422-bull_mastiff a été renommé en bull_mastiff.\n",
      "Le dossier n02108551-Tibetan_mastiff a été renommé en Tibetan_mastiff.\n",
      "Le dossier n02108915-French_bulldog a été renommé en French_bulldog.\n",
      "Le dossier n02109047-Great_Dane a été renommé en Great_Dane.\n",
      "Le dossier n02109525-Saint_Bernard a été renommé en Saint_Bernard.\n",
      "Le dossier n02109961-Eskimo_dog a été renommé en Eskimo_dog.\n",
      "Le dossier n02110063-malamute a été renommé en malamute.\n",
      "Le dossier n02110185-Siberian_husky a été renommé en Siberian_husky.\n",
      "Le dossier n02110627-affenpinscher a été renommé en affenpinscher.\n",
      "Le dossier n02110806-basenji a été renommé en basenji.\n",
      "Le dossier n02110958-pug a été renommé en pug.\n",
      "Le dossier n02111129-Leonberg a été renommé en Leonberg.\n",
      "Le dossier n02111277-Newfoundland a été renommé en Newfoundland.\n",
      "Le dossier n02111500-Great_Pyrenees a été renommé en Great_Pyrenees.\n",
      "Le dossier n02111889-Samoyed a été renommé en Samoyed.\n",
      "Le dossier n02112018-Pomeranian a été renommé en Pomeranian.\n",
      "Le dossier n02112137-chow a été renommé en chow.\n",
      "Le dossier n02112350-keeshond a été renommé en keeshond.\n",
      "Le dossier n02112706-Brabancon_griffon a été renommé en Brabancon_griffon.\n",
      "Le dossier n02113023-Pembroke a été renommé en Pembroke.\n",
      "Le dossier n02113186-Cardigan a été renommé en Cardigan.\n",
      "Le dossier n02113624-toy_poodle a été renommé en toy_poodle.\n",
      "Le dossier n02113712-miniature_poodle a été renommé en miniature_poodle.\n",
      "Le dossier n02113799-standard_poodle a été renommé en standard_poodle.\n",
      "Le dossier n02113978-Mexican_hairless a été renommé en Mexican_hairless.\n",
      "Le dossier n02115641-dingo a été renommé en dingo.\n",
      "Le dossier n02115913-dhole a été renommé en dhole.\n",
      "Le dossier n02116738-African_hunting_dog a été renommé en African_hunting_dog.\n"
     ]
    }
   ],
   "source": [
    "dossier_parent = \"C:/Users/omira/Desktop/Classify-Images-Using-Deep-Learning/Images\"\n",
    "\n",
    "# Parcourir tous les sous-dossiers\n",
    "for dossier in os.listdir(dossier_parent):\n",
    "    chemin_dossier = os.path.join(dossier_parent, dossier)\n",
    "    \n",
    "    # Vérifier si le chemin est un dossier\n",
    "    if os.path.isdir(chemin_dossier):\n",
    "        # Extraire le nouveau nom du dossier en utilisant une expression régulière\n",
    "        nouveau_nom = re.sub(r\"^\\w+-\", \"\", dossier)\n",
    "        \n",
    "        # Renommer le dossier\n",
    "        nouveau_chemin_dossier = os.path.join(dossier_parent, nouveau_nom)\n",
    "        os.rename(chemin_dossier, nouveau_chemin_dossier)\n",
    "        print(f\"Le dossier {dossier} a été renommé en {nouveau_nom}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab24375",
   "metadata": {},
   "source": [
    "# Chargement des sous dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385b9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le dossier principal contenant les images par races\n",
    "dataset_dir = \"C:/Users/omira/Desktop/Classify-Images-Using-Deep-Learning/Images\"\n",
    "\n",
    "# Liste des noms des sous-dossiers (races)\n",
    "subdirs = os.listdir(dataset_dir)\n",
    "\n",
    "# Dictionnaire pour stocker les images prétraitées (y compris l'augmentation) par race\n",
    "preprocessed_images_by_race = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7c339",
   "metadata": {},
   "source": [
    "# Traitement des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7774dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e541530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille de redimensionnement des images\n",
    "new_size = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5965471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'images augmentées par image d'origine\n",
    "num_augmented_images = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b65092d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcourir les sous-dossiers et charger les images\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(dataset_dir, subdir)\n",
    "    \n",
    "    # Vérifier si le chemin est un dossier\n",
    "    if os.path.isdir(subdir_path):\n",
    "        images = []\n",
    "        \n",
    "        # Parcourir les fichiers dans le sous-dossier\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "            \n",
    "            # Charger l'image en utilisant OpenCV\n",
    "            image = cv2.imread(file_path)\n",
    "            \n",
    "            # Vérifier si l'image est valide\n",
    "            if image is not None:\n",
    "                # Redimensionner l'image\n",
    "                image_resized = cv2.resize(image, new_size)\n",
    "                \n",
    "                # Convertir en échelle de gris\n",
    "                gray_image = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Equalisation d'histogramme\n",
    "                equalized_image = cv2.equalizeHist(gray_image)\n",
    "                \n",
    "                # Normalisation\n",
    "                normalized_image = equalized_image / 255.0\n",
    "                \n",
    "                # Whitening\n",
    "                mean = np.mean(normalized_image)\n",
    "                std = np.std(normalized_image)\n",
    "                whitened_image = (normalized_image - mean) / std\n",
    "                \n",
    "                # Ajuster la forme de l'image pour l'augmentation des données\n",
    "                whitened_image = np.expand_dims(whitened_image, axis=2)\n",
    "                \n",
    "                images.append(whitened_image)\n",
    "                \n",
    "                # Appliquer l'augmentation de données\n",
    "                augmented_images = []\n",
    "                for _ in range(num_augmented_images):\n",
    "                    augmented_image = datagen.random_transform(whitened_image)\n",
    "                    augmented_images.append(augmented_image)\n",
    "                \n",
    "                images.extend(augmented_images)\n",
    "        \n",
    "        # Stocker les images prétraitées (y compris l'augmentation) dans le dictionnaire par race\n",
    "        preprocessed_images_by_race[subdir] = images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4b38c",
   "metadata": {},
   "source": [
    "# Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "199b4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres du modèle\n",
    "input_shape = (256, 256, 1)\n",
    "num_classes = len(preprocessed_images_by_race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84af0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5001564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données d'entraînement et de validation\n",
    "train_images = []\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3cb1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer toutes les images prétraitées et les étiquettes\n",
    "for subdir, images in preprocessed_images_by_race.items():\n",
    "    train_images.extend(images)\n",
    "    train_labels.extend([subdir] * len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7837e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en tableaux numpy\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63b4b9f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.0 GiB for an array with shape (82320, 128, 128, 1) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Séparation des données en ensembles d'entraînement et de validation\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_images, validation_images, train_labels, validation_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2585\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[0;32m   2588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2589\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2587\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2581\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2583\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2586\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2587\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:356\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:185\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    184\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.0 GiB for an array with shape (82320, 128, 128, 1) and data type float64"
     ]
    }
   ],
   "source": [
    "# Séparation des données en ensembles d'entraînement et de validation\n",
    "train_images, validation_images, train_labels, validation_labels = train_test_split(\n",
    "    train_images, train_labels, test_size=0.2, stratify=train_labels, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9631b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a604175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs, validation_data=(validation_images, validation_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
